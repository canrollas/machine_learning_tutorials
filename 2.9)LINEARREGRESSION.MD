# 2.9) LINEAR REGRESSION EXAMPLES

### a) linearmodels1.py
```python
import matplotlib.pyplot as plt
import mglearn.datasets
from mglearn.plots import plot_linear_regression_wave
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

plot_linear_regression_wave()
plt.show()

X, y = mglearn.datasets.make_wave(n_samples=60)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

linear_regression = LinearRegression()
linear_regression.fit(X_train, y_train)
print("Coeefficients:{}".format(format(linear_regression.coef_[0])))
print("Intercept:{}".format(format(linear_regression.intercept_)))
print("Accuracy of the test is:{}".format(linear_regression.score(X_test, y_test)))
```

### b) linearmodels2.py

```python
from mglearn.datasets import  load_extended_boston
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
X,y = load_extended_boston()

X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)

lr = LinearRegression()
lr.fit(X_train,y_train)
print("Accuracy of the test {}".format(lr.score(X_test,y_test)))
```

- If you examine those code and outputs you will see that first script will give 0.66 accuracy.
- Why is that so small? because you can see test and train are so close. This gives underfitting.
- If you can see the second script you will get accuracy training 95 test 61 which brings overfitting.

<hr>
2022@CANROLLAS | CANROLLAS@gmail.com
<hr>
